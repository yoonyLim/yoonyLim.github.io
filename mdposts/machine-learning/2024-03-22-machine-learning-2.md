---
title: "Machine Learning 정리 노트 2"
subtitle: "Least Squares and Geometry in Machine Learning"
date: "2024-03-22"
use-math: true
---

# [Machine Learning 정리 노트 2] Least Squares and Geometry in Machine Learning

## 1. 개요

해당 정리 노트는 경희대학교 이원희 교수님의 "기계학습" 강의를 정리한 노트이다. 기계 학습의 개념은 선형대수에 대한 이해로 시작된다. 따라서 강의 내용을 본인이 이해한 내용을 바탕으로 선형대수의 개념을 풀어 설명하고자 한다.

## 2. 정리 내용

### 2.1 용어 정리

- 잔차(residual, loss, 혹 error): 실제 레이블값 $y_i$와 예측한 레이블값 $\hat{y}_i$의 사이 나타나는 차이를 의미한다. ( $ r_i = y_i - \hat{y}_i $ )
- span: 벡터들의 선형결합을 통해 만들어진 벡터 공간
- 선형 독립(linear independence): 선형결합된 벡터들의 계수들이 0일 때에만 0으로 표현될 수 있는 상태
- 선형 종속(linear dependence): 선형결합된 벡터들 중 하나의 벡터가 다른 벡터들의 선형결합으로 표현될 수 있는 상태
- 랭크(rank): 행렬의 선형 독립인 행 혹 열들의 개수
- 풀 랭크(full rank): 행렬이 가질 수 있는 최대 랭크, 즉 선형 독립인 최대 행 혹 열들의 개수 (예시: $n \times p$의 행렬 $X$에 대해 $n \ge p$이면 $X$의 풀 랭크는 $p$)

### 2.2 최소 제곱/자승법

[저번 포스트](https://yoonylim.github.io/posts/machine-learning/2024-03-21-machine-learning-1)를 통해 우리는 예측되는 레이블 벡터는 $\hat{y} = Xw$로 표현될 수 있다는 것을 알고 있다. 그렇다면 가중치 벡터 $w$는 어떻게 결정할 수 있을까?

해당 질문에 대한 답으로 최소 제곱법 혹 최소 자승법으로 불리는 방법을 사용한다. 잔차는 $ r_i = y_i - \hat{y}_i $로 표현되고 이 중 잔차와 예측하는 레이블 벡터는 가중치 벡터 $w$에 의존적이므로 $ r_i(w) = y_i - \hat{y}_i(w) $로도 표현한다. 즉, 가중치 벡터 $w$의 각 가중치들을 조절하여 실제 레이블 값과 예측 레이블 값의 차이를 줄여 가장 작은 잔차를 이룰 수 있다.

이를 기하학적으로 표현하자면 잔차 벡터는 각 잔차들로 구성된 벡터로 $r = \begin{bmatrix} r_1 \\\\ r_2 \\\\ \vdots \\\\ r_n \end{bmatrix}$로 표현될 때 결국 이는 실제 레이블 벡터 $y$와 예측 레이블 벡터 $\hat{y}$의 차이인 벡터로 두 벡터 사이의 거리를 나타낸다. 이전 포스트에서 다룬 거리를 나타내는 $L^2$ norm의 제곱을 사용하여 표현하면 $ \lVert r(w) \rVert ^2 = \lVert y - \hat{y}(w) \rVert ^2 =
\sum_{i=1}^n ( y_i - \langle w, x_i \rangle ) ^2$으로 표현할 수 있다. 이는 $n$차원에서 적용되는 피타고라스 정리의 확장된 적용이다.

이때, 최소 제곱법의 이론 이해를 위해 하나의 가정을 한다. 해당 가정은 특성 행렬 $X$의 모든 특성 벡터들이 서로 선형 독립이라는 것이다. 그렇다면 $\hat{y} = Xw = w_1 x_1 + w_2 x_2 + \dots + w_p x_p$이고 이는 $\hat{y}$가 행렬 $X$의 열벡터들로 이루어지는 벡터 공간에 속한다는 것을 의미한다. $\hat{y}$가 $x_1$, $x_2$, $\dots$, $x_p$로 표현 가능하므로 $\hat{y} \in \text{span}(\text{cols}(X))$이다.

행렬 $X$가 2개의 특성 벡터를 지닌다 가정하면 아래 그림과 같이 실제 레이블 벡터 $y$, 최적의 예측 레이블 벡터 $\hat{y}$, 가장 작은 잔차 벡터 $r$, 벡터 공간 $\text{span}(\text{cols}(X))$과의 관계를 나타낼 수 있다. 이때 가장 작은 잔차 벡터라 하면 실제 레이블 벡터와 벡터 공간 간의 거리가 가장 작을 때의 예측 레이블 벡터를 의미하기에 $r$은 벡터 공간에 수직일 수밖에 없다.

[img0]

그리고 아직 최적에 이르지 못한 예측 레이블 벡터 $\tilde{y}$, 이때의 실제 레이블 벡터와의 잔차 벡터 $\tilde{r}$을 추가하면 $\hat{y}$와 $\tilde{y}$의 차이 벡터를 $d$라 했을 때 아래와 같은 관계가 완성된다. 

[img1]

위 그림에서 보이듯 잔차를 줄여나가 최적의 상태에 도달하도록 해야 한다.